<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>Gemini 2.5 Flash - Chatbot Interface</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 20px; display: flex; flex-direction: column; height: calc(100vh - 40px); }
        h3 { margin-top: 0; }
        .chat-container { flex: 1; overflow-y: auto; border: 1px solid #ccc; padding: 10px; margin-bottom: 10px; background: #f5f5f5; }
        .message { margin: 10px 0; padding: 8px 12px; border-radius: 5px; }
        .user-message { background: #e3f2fd; margin-left: 20%; text-align: right; }
        .assistant-message { background: #f5f5f5; margin-right: 20%; border: 1px solid #ddd; }
        .assistant-message pre { margin: 0; white-space: pre-wrap; word-wrap: break-word; font-family: monospace; font-size: 12px; }
        .input-container { display: flex; gap: 10px; }
        #userInput { flex: 1; padding: 10px; font-size: 14px; height: 100px; resize: none; overflow-y: auto; font-family: Arial, sans-serif; }
        button { padding: 10px 20px; cursor: pointer; }
        .status { margin: 10px 0; padding: 5px; border-radius: 3px; }
        .success { background: #d4edda; color: #155724; }
        .error { background: #f8d7da; color: #721c24; }
        .info { background: #cce7ff; color: #004085; }
        #timer { font-weight: bold; margin: 10px 0; color: #333; }
        .transcription-container {
            border: 1px solid #ddd;
            padding: 10px;
            margin: 10px 0;
            min-height: 100px;
            max-height: 200px;
            overflow-y: auto;
            background: #f8f9fa;
            font-family: Arial, sans-serif;
            font-size: 14px;
        }
        .interim { color: #666; font-style: italic; }
        .final { color: #000; }
        .mute-controls { margin: 5px 0; display: flex; align-items: center; gap: 10px; }
        .mute-controls input[type="checkbox"] { cursor: pointer; }
        .mute-controls label { cursor: pointer; user-select: none; }
    </style>
</head>
<body>
    <h3>Gemini 2.5 Flash - Chatbot Interface</h3>
    <div id="knowledgeUpload" style="margin-bottom: 20px; padding: 15px; background: #f0f8ff; border: 1px solid #4CAF50; border-radius: 5px;">
        <label for="fileInput" style="display: block; margin-bottom: 10px; font-weight: bold;">Upload Knowledge Base (Text File):</label>
        <input type="file" id="fileInput" accept=".txt" style="margin-bottom: 10px;">
        <div id="fileInfo" style="display: none; margin-top: 10px; padding: 10px; background: #e8f5e9; border-radius: 3px;">
            <strong>Loaded:</strong> <span id="fileName"></span> (<span id="fileSize"></span>)
        </div>
    </div>
    <div id="chatContainer" class="chat-container"></div>
    <div id="timer"></div>
    <!-- Cache info commented out until cachedContentTokenCount is fixed
    <div id="cacheInfo" style="margin: 10px 0; padding: 10px; background: #fff3cd; border: 1px solid #ffc107; border-radius: 3px; display: none;">
        <strong>Cache Stats:</strong> <span id="cacheStats"></span>
    </div>
    -->
    <div id="status" class="status" style="display: none;"></div>
    <div class="mute-controls">
        <input type="checkbox" id="muteCheckbox" checked>
        <label for="muteCheckbox">Mute Transcription</label>
    </div>
    <div class="transcription-container">
        <div id="liveTranscription">
            <span class="interim">Transcription will appear here when unmuted...</span>
        </div>
    </div>
    <div class="input-container">
        <textarea id="userInput" placeholder="Type your message... (press Enter 3 times to send)" disabled></textarea>
        <button onclick="clearChat()">Clear Chat</button>
    </div>
    
    <script>
        let conversationHistory = [];
        let requestStartTime = 0;
        let isStreaming = false;
        let knowledgeBaseContent = '';
        let knowledgeBaseFileName = '';
        let currentAbortController = null;
        let enterPressCount = 0;
        
        // Transcription variables
        let ws = null;
        let microphone = null;
        let isTranscribing = false;
        let utterances = new Map();
        let microphoneStream = null;
        let finalizedTranscription = ''; // Store finalized text separately
        
        const userInput = document.getElementById('userInput');
        const chatContainer = document.getElementById('chatContainer');
        const timerElement = document.getElementById('timer');
        const muteCheckbox = document.getElementById('muteCheckbox');
        const liveTranscription = document.getElementById('liveTranscription');
        
        // Enable input on load
        window.onload = async () => {
            userInput.disabled = false;
            userInput.focus();
            setupFileUpload();
            
            // Request microphone permissions on load
            try {
                console.log('Requesting microphone permissions on page load...');
                microphoneStream = await navigator.mediaDevices.getUserMedia({ 
                    audio: {
                        echoCancellation: true,
                        noiseSuppression: true,
                        sampleRate: 16000
                    }
                });
                console.log('Microphone access granted on page load');
                // Stop the stream immediately - we'll use it later when needed
                microphoneStream.getTracks().forEach(track => track.stop());
                
                // Setup mute checkbox listener
                muteCheckbox.addEventListener('change', handleMuteChange);
            } catch (error) {
                console.error('Failed to get microphone permissions:', error);
                showStatus('Microphone access denied. Voice transcription will not be available.', 'error');
            }
        };
        
        function setupFileUpload() {
            const fileInput = document.getElementById('fileInput');
            fileInput.addEventListener('change', async (event) => {
                const file = event.target.files[0];
                if (file && file.type === 'text/plain') {
                    try {
                        knowledgeBaseContent = await file.text();
                        knowledgeBaseFileName = file.name;
                        
                        // Update UI
                        document.getElementById('fileName').textContent = file.name;
                        document.getElementById('fileSize').textContent = formatFileSize(file.size);
                        document.getElementById('fileInfo').style.display = 'block';
                        
                        showStatus(`Knowledge base "${file.name}" loaded successfully! (${knowledgeBaseContent.length} characters)`, 'success');
                    } catch (error) {
                        showStatus(`Error reading file: ${error.message}`, 'error');
                    }
                } else {
                    showStatus('Please select a text file (.txt)', 'error');
                    event.target.value = '';
                }
            });
        }
        
        function formatFileSize(bytes) {
            if (bytes < 1024) return bytes + ' bytes';
            else if (bytes < 1024 * 1024) return Math.round(bytes / 1024) + ' KB';
            else return Math.round(bytes / (1024 * 1024) * 10) / 10 + ' MB';
        }
        
        function fixIncompleteJSON(partialJSON) {
            // Try to parse as-is first
            try {
                return JSON.parse(partialJSON);
            } catch (e) {
                // Not valid JSON, attempt to fix it
            }
            
            let fixed = partialJSON.trim();
            
            // Check if we're in the middle of a string by counting quotes
            let quoteCount = 0;
            let inString = false;
            let lastChar = '';
            
            for (let i = 0; i < fixed.length; i++) {
                const char = fixed[i];
                if (char === '"' && lastChar !== '\\') {
                    quoteCount++;
                    inString = !inString;
                }
                lastChar = char;
            }
            
            // If we're in the middle of a string, close it
            if (inString) {
                fixed += '"';
            }
            
            // Check if we need to close the array
            if (!fixed.includes('"]')) {
                // Find the last occurrence of sentences array
                const sentencesIndex = fixed.lastIndexOf('"sentences"');
                if (sentencesIndex !== -1) {
                    // Check if array is not closed
                    const afterSentences = fixed.substring(sentencesIndex);
                    if (afterSentences.includes('[') && !afterSentences.includes(']')) {
                        fixed += ']';
                    }
                }
            }
            
            // Check if we need to close the object
            if (!fixed.endsWith('}')) {
                fixed += '}';
            }
            
            // Try to parse the fixed JSON
            try {
                const parsed = JSON.parse(fixed);
                // Add interruption marker if sentences array exists
                if (parsed.sentences && Array.isArray(parsed.sentences)) {
                    parsed.sentences.push('(Response interrupted by user)');
                }
                return parsed;
            } catch (e) {
                // If still can't parse, return a default structure
                return {
                    sentences: ['(Response interrupted by user - partial response could not be recovered)']
                };
            }
        }
        
        document.addEventListener('keydown', (e) => {
            // Always focus textarea if not already focused
            if (document.activeElement !== userInput && !userInput.disabled) {
                userInput.focus();
            }
            
            if (e.key === 'Tab' && document.activeElement === userInput) {
                e.preventDefault();
                const start = userInput.selectionStart;
                const end = userInput.selectionEnd;
                const value = userInput.value;
                userInput.value = value.substring(0, start) + '\t' + value.substring(end);
                userInput.selectionStart = userInput.selectionEnd = start + 1;
                enterPressCount = 0;
            } else if (e.key === 'Enter') {
                if (isStreaming) {
                    // Single Enter to interrupt current stream
                    e.preventDefault();
                    if (currentAbortController) {
                        currentAbortController.abort();
                        enterPressCount = 0;
                    }
                } else if (document.activeElement === userInput) {
                    e.preventDefault();
                    const content = userInput.value;
                    const transcriptionContent = liveTranscription.textContent.trim();
                    
                    // Check if there's content in either typed textarea or transcription
                    const hasTypedContent = content.length > 0;
                    const hasTranscriptionContent = transcriptionContent.length > 0 && 
                        !transcriptionContent.includes('Transcription will appear here') &&
                        !transcriptionContent.includes('Listening...');
                    
                    // If neither has content, do nothing
                    if (!hasTypedContent && !hasTranscriptionContent) {
                        return;
                    }
                    
                    enterPressCount++;
                    
                    if (enterPressCount === 3) {
                        // Submit on third Enter press
                        sendMessage();
                        enterPressCount = 0;
                    } else {
                        // Insert newline for 1st and 2nd Enter press
                        // Only insert newline if the typed textarea has content
                        if (hasTypedContent) {
                            const start = userInput.selectionStart;
                            const end = userInput.selectionEnd;
                            userInput.value = content.substring(0, start) + '\n' + content.substring(end);
                            userInput.selectionStart = userInput.selectionEnd = start + 1;
                        }
                    }
                }
            } else if (e.key === 'Escape') {
                // Toggle mute checkbox on Esc key
                e.preventDefault();
                muteCheckbox.checked = !muteCheckbox.checked;
                // Trigger change event to call handleMuteChange
                muteCheckbox.dispatchEvent(new Event('change'));
            } else {
                // Reset counter on any non-Enter key
                enterPressCount = 0;
            }
        });
        
        function showStatus(message, type = 'info') {
            const statusDiv = document.getElementById('status');
            statusDiv.textContent = message;
            statusDiv.className = `status ${type}`;
            statusDiv.style.display = 'block';
        }
        
        function addMessage(role, content) {
            const messageDiv = document.createElement('div');
            messageDiv.className = `message ${role === 'user' ? 'user-message' : 'assistant-message'}`;
            
            if (role === 'user') {
                messageDiv.textContent = content;
            } else {
                const pre = document.createElement('pre');
                pre.textContent = content;
                messageDiv.appendChild(pre);
            }
            
            chatContainer.appendChild(messageDiv);
            chatContainer.scrollTop = chatContainer.scrollHeight;
        }
        
        async function sendMessage() {
            const typedMessage = userInput.value.trim();
            const transcribedMessage = liveTranscription.textContent.trim();
            
            // Remove the default placeholder text
            const isPlaceholder = transcribedMessage.includes('Transcription will appear here') || 
                                  transcribedMessage.includes('Listening...');
            const cleanTranscription = isPlaceholder ? '' : transcribedMessage;
            
            // Combine messages
            let message = '';
            if (cleanTranscription && typedMessage) {
                message = `Transcribed: ${cleanTranscription}\nTyped: ${typedMessage}`;
            } else if (cleanTranscription) {
                message = `Transcribed: ${cleanTranscription}`;
            } else if (typedMessage) {
                message = typedMessage;
            } else {
                return; // Nothing to send
            }
            
            // Disable input during streaming
            isStreaming = true;
            userInput.disabled = true;
            userInput.value = '';
            userInput.placeholder = 'Press Enter to interrupt...';
            
            // Clear transcription
            clearTranscription();
            
            // Stop transcription during response streaming
            if (isTranscribing) {
                console.log('Stopping transcription for response streaming');
                stopTranscription();
            }
            
            // Add user message to UI and history
            addMessage('user', message);
            conversationHistory.push({
                role: 'user',
                parts: [{ text: message }]
            });
            
            // Start timer
            requestStartTime = performance.now();
            timerElement.textContent = 'Timer: Sending request to Gemini...';
            showStatus('Processing with streaming JSON output...', 'info');
            
            // Create placeholder for assistant response
            const assistantDiv = document.createElement('div');
            assistantDiv.className = 'message assistant-message';
            const pre = document.createElement('pre');
            assistantDiv.appendChild(pre);
            
            // Create audio element for TTS
            const audioElement = document.createElement('audio');
            audioElement.controls = true;
            audioElement.style.width = '100%';
            audioElement.style.marginTop = '10px';
            audioElement.style.display = 'none'; // Hidden until TTS starts
            assistantDiv.appendChild(audioElement);
            
            chatContainer.appendChild(assistantDiv);
            
            let fullResponse = '';
            let firstChunkTime = null;
            
            try {
                // Create new AbortController for this request
                currentAbortController = new AbortController();
                
                const requestBody = {
                    contents: conversationHistory,
                    structuredOutput: true
                };
                
                // Build system instruction with or without knowledge base
                const knowledgeFileName = knowledgeBaseContent ? knowledgeBaseFileName : '(none loaded)';
                const knowledgeContent = knowledgeBaseContent || '[No content - no knowledge base file has been uploaded]';
                
                if (knowledgeBaseContent) {
                    console.log('Knowledge base loaded:', knowledgeBaseFileName, 'Length:', knowledgeBaseContent.length);
                } else {
                    console.log('No knowledge base loaded');
                }
                
                requestBody.systemInstruction = `You are a helpful assistant. The user may communicate with you through both typing and voice transcription. When you see messages prefixed with "Transcribed:" it means they spoke the message, and "Typed:" means they typed it. Sometimes messages may contain both transcribed and typed content.

When answering questions, refer to the knowledge base when relevant. If the user asks about something not in the knowledge base, you must state that it's not discussed in the provided content, but then provide an answer based on your general knowledge. For example: "This topic is not discussed in ${knowledgeFileName}, but based on my general knowledge..."

You must respond with a JSON object containing a "sentences" array. IMPORTANT: Each item in the array must contain EXACTLY ONE sentence. Never put multiple sentences in a single array item. Split your response so that each sentence (ending with . ! or ?) is its own array element.

Be conversational, friendly, and helpful in your responses.

Knowledge base content from "${knowledgeFileName}":

${knowledgeContent}`;
                
                const response = await fetch('/api/gemini', {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify(requestBody),
                    signal: currentAbortController.signal
                });
                
                if (!response.ok) {
                    throw new Error(`API Error: ${response.status}`);
                }
                
                const reader = response.body.getReader();
                const decoder = new TextDecoder();
                let buffer = '';
                
                while (true) {
                    const { done, value } = await reader.read();
                    if (done) break;
                    
                    buffer += decoder.decode(value, { stream: true });
                    const lines = buffer.split('\n');
                    buffer = lines.pop() || '';
                    
                    for (const line of lines) {
                        if (line.startsWith('data: ')) {
                            const data = line.slice(6);
                            if (data === '[DONE]') {
                                const totalTime = Math.round(performance.now() - requestStartTime);
                                timerElement.textContent = `Timer: ${totalTime}ms total (first chunk: ${firstChunkTime}ms)`;
                                showStatus('Streaming complete!', 'success');
                                
                                // Add complete response to conversation history
                                conversationHistory.push({
                                    role: 'model',
                                    parts: [{ text: fullResponse }]
                                });
                                
                                // Trigger TTS conversion
                                console.log('=== TTS TRIGGER AFTER [DONE] ===');
                                console.log('Full response to parse:', fullResponse);
                                
                                try {
                                    const parsedResponse = JSON.parse(fullResponse);
                                    console.log('TTS: Successfully parsed JSON:', parsedResponse);
                                    
                                    const sentences = parsedResponse.sentences || [];
                                    console.log('TTS: Sentences array count:', sentences.length);
                                    console.log('TTS: Sentences:', sentences);
                                    
                                    if (sentences.length > 0) {
                                        const textForTTS = sentences.join(' ');
                                        console.log('TTS: Concatenated text for TTS:', textForTTS);
                                        console.log('TTS: Audio element for this response:', audioElement);
                                        
                                        // Test async operations in streaming context
                                        console.log('TTS: Testing async operations in streaming context...');
                                        
                                        // Test 1: setTimeout works?
                                        setTimeout(() => {
                                            console.log('TTS: setTimeout executed successfully');
                                        }, 100);
                                        
                                        // Test 2: Promise resolution works?
                                        Promise.resolve().then(() => {
                                            console.log('TTS: Promise.resolve executed successfully');
                                        });
                                        
                                        // Test 3: Fetch to a different endpoint?
                                        fetch('/').then(() => {
                                            console.log('TTS: Fetch to root path succeeded');
                                        }).catch(e => {
                                            console.log('TTS: Fetch to root path failed:', e);
                                        });
                                        
                                        // Test 4: XMLHttpRequest works?
                                        // Commenting out to see if this is blocking Test 5
                                        // const xhr = new XMLHttpRequest();
                                        // xhr.open('GET', '/', true);
                                        // xhr.onload = () => console.log('TTS: XMLHttpRequest succeeded');
                                        // xhr.onerror = () => console.log('TTS: XMLHttpRequest failed');
                                        // xhr.send();
                                        
                                        // Test 5: Direct fetch to TTS endpoint
                                        // Removed - this hanging fetch might be blocking subsequent TTS calls
                                        
                                        // Start TTS conversion (deferred to break out of streaming context)
                                        // Minimum delay needed: 625ms (works), 562ms or less hangs
                                        setTimeout(() => {
                                            console.log('TTS: Deferred execution starting after 625ms delay...');
                                            convertTextToSpeech(textForTTS, audioElement);
                                        }, 625);
                                    } else {
                                        console.warn('TTS: No sentences found in response');
                                    }
                                } catch (error) {
                                    console.error('TTS: Error parsing response for TTS:', error);
                                    console.error('TTS: Response that failed to parse:', fullResponse);
                                }
                            } else {
                                try {
                                    const parsed = JSON.parse(data);
                                    if (parsed.text) {
                                        if (!firstChunkTime) {
                                            firstChunkTime = Math.round(performance.now() - requestStartTime);
                                            timerElement.textContent = `Timer: ${firstChunkTime}ms to first chunk (streaming...)`;
                                        }
                                        fullResponse += parsed.text;
                                        // Show the raw response (which should be JSON)
                                        pre.textContent = fullResponse;
                                        chatContainer.scrollTop = chatContainer.scrollHeight;
                                    } else if (parsed.cacheInfo) {
                                        // Cache info commented out until cachedContentTokenCount is fixed
                                        // const cacheDiv = document.getElementById('cacheInfo');
                                        // const cacheStats = document.getElementById('cacheStats');
                                        // cacheStats.textContent = `${parsed.cacheInfo.cachedTokens} cached tokens out of ${parsed.cacheInfo.totalTokens} total (${Math.round(parsed.cacheInfo.cacheHitRate * 100)}% cache hit rate)`;
                                        // cacheDiv.style.display = 'block';
                                    } else if (parsed.error) {
                                        throw new Error(parsed.error);
                                    }
                                } catch (e) {
                                    console.error('Error parsing SSE data:', e);
                                }
                            }
                        }
                    }
                }
            } catch (error) {
                if (error.name === 'AbortError') {
                    // Handle user interruption
                    showStatus('Response interrupted', 'info');
                    const totalTime = Math.round(performance.now() - requestStartTime);
                    timerElement.textContent = `Timer: ${totalTime}ms (interrupted)`;
                    
                    // Fix incomplete JSON and add to conversation history
                    if (fullResponse) {
                        console.log('=== TTS TRIGGER FOR INTERRUPTED RESPONSE ===');
                        console.log('TTS: Processing interrupted response');
                        const fixedResponse = fixIncompleteJSON(fullResponse);
                        console.log('TTS: Fixed response structure:', fixedResponse);
                        
                        // Update the display with the fixed JSON
                        pre.textContent = JSON.stringify(fixedResponse, null, 2);
                        
                        // Add the interrupted response to conversation history
                        conversationHistory.push({
                            role: 'model',
                            parts: [{ text: JSON.stringify(fixedResponse) }]
                        });
                        
                        // Trigger TTS for interrupted response
                        const sentences = fixedResponse.sentences || [];
                        console.log('TTS: Sentences from interrupted response:', sentences);
                        
                        if (sentences.length > 0) {
                            const textForTTS = sentences.join(' ');
                            console.log('TTS: Concatenated text for interrupted TTS:', textForTTS);
                            console.log('TTS: Audio element for interrupted response:', audioElement);
                            
                            // Start TTS conversion (deferred to break out of streaming context)
                            setTimeout(() => {
                                convertTextToSpeech(textForTTS, audioElement);
                            }, 0);
                        } else {
                            console.warn('TTS: No sentences found in interrupted response');
                        }
                    } else {
                        // No response received before interruption
                        const emptyResponse = { sentences: ['(Response interrupted before any content was received)'] };
                        pre.textContent = JSON.stringify(emptyResponse, null, 2);
                        conversationHistory.push({
                            role: 'model',
                            parts: [{ text: JSON.stringify(emptyResponse) }]
                        });
                    }
                } else {
                    // Handle other errors
                    console.error('Error:', error);
                    showStatus(`Error: ${error.message}`, 'error');
                    timerElement.textContent = 'Timer: Request failed';
                    pre.textContent = `Error: ${error.message}`;
                }
            } finally {
                // Re-enable input and clear abort controller
                isStreaming = false;
                userInput.disabled = false;
                userInput.placeholder = 'Type your message... (press Enter 3 times to send)';
                userInput.focus();
                currentAbortController = null;
                enterPressCount = 0;
                
                // Restart transcription if unmuted
                if (!muteCheckbox.checked) {
                    console.log('Restarting transcription after response');
                    // Add longer delay to allow server cleanup
                    setTimeout(() => {
                        console.log('Starting transcription after delay');
                        startTranscription();
                    }, 1000);
                }
            }
        }
        
        function clearChat() {
            conversationHistory = [];
            chatContainer.innerHTML = '';
            timerElement.textContent = '';
            document.getElementById('status').style.display = 'none';
            userInput.focus();
        }
        
        // Handle mute checkbox changes
        function handleMuteChange() {
            const isMuted = muteCheckbox.checked;
            console.log('Mute checkbox changed, muted:', isMuted);
            
            if (isMuted) {
                // Stop transcription if running
                if (isTranscribing) {
                    stopTranscription();
                }
            } else {
                // Start transcription if not streaming a response
                if (!isStreaming) {
                    startTranscription();
                }
            }
        }
        
        // Start transcription with retry capability
        async function startTranscription(retryCount = 0) {
            // Prevent overlapping connections
            if (ws && (ws.readyState === WebSocket.CONNECTING || ws.readyState === WebSocket.OPEN)) {
                console.log('WebSocket connection already exists, skipping...');
                return;
            }
            
            try {
                console.log(`[STT WebSocket] Starting transcription... (attempt ${retryCount + 1})`);
                showStatus('Connecting to transcription server...', 'info');
                
                // Don't clear transcription content - preserve existing text
                // Only show "Listening..." if there's no existing content
                if (!liveTranscription.textContent.trim() || 
                    liveTranscription.textContent.includes('Transcription will appear here')) {
                    liveTranscription.innerHTML = '<span class="interim">Listening...</span>';
                }
                
                // Connect to server WebSocket
                const protocol = window.location.protocol === 'https:' ? 'wss:' : 'ws:';
                const wsUrl = `${protocol}//${window.location.hostname}:3000/api/speech-to-text`;
                console.log('[STT WebSocket] Attempting to connect to:', wsUrl);
                
                ws = new WebSocket(wsUrl);
                console.log('[STT WebSocket] Created, state:', ws.readyState);
                
                ws.onopen = () => {
                    console.log('[STT WebSocket] Opened successfully');
                    showStatus('Connected to transcription server', 'success');
                    
                    // Send configuration to start Deepgram connection
                    const config = {
                        action: 'start',
                        model: 'nova-3-medical'
                    };
                    ws.send(JSON.stringify(config));
                    
                    // Setup microphone after connection
                    setTimeout(() => setupMicrophone(), 100);
                };
                
                ws.onmessage = (event) => {
                    // console.log('Received message from server:', event.data);
                    try {
                        const data = JSON.parse(event.data);
                        
                        if (data.type === 'status') {
                            console.log('Status message:', data.message);
                            showStatus(data.message, 'info');
                        } else if (data.type === 'error') {
                            console.error('Error from server:', data.message);
                            showStatus(`Transcription error: ${data.message}`, 'error');
                            stopTranscription();
                        } else {
                            // Handle transcription data
                            console.log('Transcription data:', data);
                            updateTranscription(data);
                        }
                    } catch (error) {
                        console.error('Error parsing message:', error);
                    }
                };
                
                ws.onerror = (error) => {
                    console.error('[STT WebSocket] Error:', error);
                    console.error('[STT WebSocket] State on error:', ws.readyState);
                    
                    // Retry with exponential backoff
                    if (retryCount < 3) {
                        const retryDelay = Math.pow(2, retryCount) * 1000; // 1s, 2s, 4s
                        console.log(`Retrying connection in ${retryDelay}ms...`);
                        showStatus(`Connection failed, retrying in ${retryDelay/1000}s...`, 'warning');
                        
                        // Clean up current connection
                        if (ws) {
                            ws.close();
                            ws = null;
                        }
                        isTranscribing = false;
                        
                        setTimeout(() => {
                            if (!isStreaming && !muteCheckbox.checked) {
                                startTranscription(retryCount + 1);
                            }
                        }, retryDelay);
                    } else {
                        console.error('Max retries reached, giving up');
                        showStatus('Failed to connect after multiple attempts', 'error');
                        stopTranscription();
                    }
                };
                
                ws.onclose = (event) => {
                    console.log('[STT WebSocket] Closed. Code:', event.code, 'Reason:', event.reason);
                    console.log('Clean close?', event.wasClean);
                    
                    // Clean up WebSocket reference
                    ws = null;
                    isTranscribing = false;
                    
                    if (event.code === 1006 && retryCount < 3) {
                        console.error('Abnormal closure - attempting retry');
                        const retryDelay = Math.pow(2, retryCount) * 1000;
                        showStatus(`Connection lost, retrying in ${retryDelay/1000}s...`, 'warning');
                        
                        setTimeout(() => {
                            if (!isStreaming && !muteCheckbox.checked) {
                                startTranscription(retryCount + 1);
                            }
                        }, retryDelay);
                    } else if (event.code === 1006) {
                        console.error('Abnormal closure - connection lost without proper close handshake');
                        showStatus('Transcription connection lost abnormally', 'error');
                    } else if (isTranscribing) {
                        showStatus('Transcription connection closed', 'warning');
                    }
                };
                
                // Update state
                isTranscribing = true;
                
            } catch (error) {
                console.error('Failed to start transcription:', error);
                showStatus(`Failed to start transcription: ${error.message}`, 'error');
                stopTranscription();
            }
        }
        
        // Setup microphone
        async function setupMicrophone() {
            try {
                console.log('Setting up microphone...');
                const stream = await navigator.mediaDevices.getUserMedia({ 
                    audio: {
                        echoCancellation: true,
                        noiseSuppression: true,
                        sampleRate: 16000
                    }
                });
                console.log('Microphone stream obtained');
                
                const mimeType = 'audio/webm';
                console.log('Using mime type:', mimeType);
                
                microphone = new MediaRecorder(stream, {
                    mimeType: mimeType
                });
                
                microphone.ondataavailable = (event) => {
                    if (event.data.size > 0 && ws && ws.readyState === WebSocket.OPEN) {
                        // console.log('Sending audio chunk:', event.data.size, 'bytes');
                        ws.send(event.data);
                    }
                };
                
                microphone.onerror = (error) => {
                    console.error('MediaRecorder error:', error);
                    showStatus('Microphone error', 'error');
                };
                
                // Start recording with 100ms chunks
                microphone.start(100);
                console.log('MediaRecorder started');
                showStatus('Listening...', 'success');
                
            } catch (error) {
                console.error('Microphone setup error:', error);
                showStatus('Microphone access denied', 'error');
                stopTranscription();
            }
        }
        
        // Stop transcription
        function stopTranscription() {
            console.log('Stopping transcription...');
            
            // Stop microphone
            if (microphone && microphone.state !== 'inactive') {
                microphone.stop();
                microphone.stream.getTracks().forEach(track => track.stop());
                microphone = null;
                console.log('Microphone stopped');
            }
            
            // Close WebSocket connection
            if (ws) {
                if (ws.readyState === WebSocket.OPEN) {
                    ws.send(JSON.stringify({ action: 'stop' }));
                }
                ws.close();
                ws = null;
                console.log('WebSocket closed');
            }
            
            // Update state
            isTranscribing = false;
            
            // Don't clear transcription - let it persist
            // Only show placeholder if there's truly no content
            if (utterances.size === 0 && !finalizedTranscription) {
                liveTranscription.innerHTML = '<span class="interim">Transcription will appear here when unmuted...</span>';
            }
        }
        
        // Update transcription display
        function updateTranscription(data) {
            if (data.type === 'SpeechStarted' || data.type === 'UtteranceEnd') {
                return;
            }
            
            if (!data.channel || !data.channel.alternatives || !data.channel.alternatives[0]) {
                console.log('Unexpected data format:', data);
                return;
            }
            
            const transcript = data.channel.alternatives[0].transcript;
            if (transcript === "") return;
            
            const isFinal = data.is_final;
            const utteranceId = data.start;
            
            if (utteranceId === undefined || utteranceId === null) {
                console.warn("Received transcript without a start time, cannot process.", data);
                return;
            }
            
            if (isFinal) {
                // If this utterance was previously interim, remove it from the map
                if (utterances.has(utteranceId)) {
                    utterances.delete(utteranceId);
                }
                // Append finalized text
                if (finalizedTranscription && !finalizedTranscription.endsWith(' ')) {
                    finalizedTranscription += ' ';
                }
                finalizedTranscription += transcript;
            } else {
                // Store or update interim transcription
                utterances.set(utteranceId, {
                    transcript: transcript,
                    is_final: false
                });
            }
            
            // Build display content: finalized text + interim utterances
            let displayContent = '';
            
            // Add finalized transcription
            if (finalizedTranscription) {
                displayContent += `<span class="final">${finalizedTranscription}</span>`;
            }
            
            // Add interim utterances (sorted by ID for consistency within current session)
            const sortedUtterances = Array.from(utterances.entries()).sort((a, b) => a[0] - b[0]);
            for (const [id, utterance] of sortedUtterances) {
                if (!utterance.is_final) {
                    if (displayContent && !displayContent.endsWith(' ')) {
                        displayContent += ' ';
                    }
                    displayContent += `<span class="interim">${utterance.transcript}</span>`;
                }
            }
            
            // Update display
            liveTranscription.innerHTML = displayContent || '<span class="interim">Listening...</span>';
            liveTranscription.scrollTop = liveTranscription.scrollHeight;
        }
        
        // Clear transcription
        function clearTranscription() {
            liveTranscription.innerHTML = '<span class="interim">Transcription will appear here when unmuted...</span>';
            utterances.clear();
            finalizedTranscription = '';
        }
        
        // Convert text to speech using ElevenLabs
        async function convertTextToSpeech(text, audioElement) {
            console.log('=== TTS CONVERSION STARTED ===');
            console.log('Text length:', text.length);
            console.log('First 100 chars:', text.substring(0, 100));
            console.log('Audio element:', audioElement);
            
            const DEFAULT_VOICE_ID = "21m00Tcm4TlvDq8ikWAM";
            
            if (!text || !audioElement) {
                console.error('TTS Error: Missing text or audio element');
                return;
            }
            
            // Check MediaSource support for MP3
            if (!('MediaSource' in window)) {
                console.error('TTS Error: MediaSource not supported');
                showStatus('Text-to-speech not supported in this browser', 'error');
                return;
            }
            
            const mimeType = 'audio/mpeg';
            if (!MediaSource.isTypeSupported(mimeType)) {
                console.error('TTS Error: Browser does not support MP3 with MediaSource');
                showStatus('Browser does not support MP3 streaming', 'error');
                return;
            }
            
            console.log('TTS: MediaSource supported, starting stream...');
            
            // Test fetch before MediaSource
            // Removed - this hanging fetch might be blocking subsequent TTS calls
            
            try {
                const mediaSource = new MediaSource();
                audioElement.src = URL.createObjectURL(mediaSource);
                audioElement.style.display = 'block';
                
                let sourceBuffer;
                let queue = [];
                let isAppending = false;
                let hasStartedPlaying = false;
                
                function processQueue() {
                    if (queue.length > 0 && !isAppending && sourceBuffer && !sourceBuffer.updating) {
                        isAppending = true;
                        const chunk = queue.shift();
                        sourceBuffer.appendBuffer(chunk);
                    }
                }
                
                mediaSource.addEventListener('sourceopen', async () => {
                    console.log('TTS: MediaSource opened');
                    sourceBuffer = mediaSource.addSourceBuffer(mimeType);
                    
                    sourceBuffer.addEventListener('updateend', () => {
                        isAppending = false;
                        processQueue();
                        
                        // Try to start playback after first chunk
                        if (!hasStartedPlaying && audioElement.buffered.length > 0) {
                            hasStartedPlaying = true;
                            console.log('TTS: First chunk buffered, attempting play...');
                            audioElement.play()
                                .then(() => {
                                    console.log('TTS: Playback started successfully');
                                })
                                .catch(e => {
                                    console.error('TTS: Play error:', e);
                                    console.error('TTS: Play error details:', {
                                        name: e.name,
                                        message: e.message,
                                        stack: e.stack
                                    });
                                    showStatus('Click play button to hear response', 'info');
                                });
                        }
                    });
                    
                    // Start fetching and streaming
                    console.log('TTS: Sending request to /api/text-to-speech');
                    console.log('TTS: Request body:', { text: text.substring(0, 50) + '...', voiceId: DEFAULT_VOICE_ID });
                    
                    console.log('TTS: audioElement check:', audioElement);
                    console.log('TTS: audioElement in DOM?', document.contains(audioElement));
                    console.log('TTS: About to call fetch()...');
                    let response;
                    try {
                        response = await fetch('/api/text-to-speech', {
                            method: 'POST',
                            headers: { 'Content-Type': 'application/json' },
                            body: JSON.stringify({ text: text, voiceId: DEFAULT_VOICE_ID })
                        });
                        console.log('TTS: Fetch completed, got response:', response);
                        console.log('TTS: Response status:', response.status);
                        console.log('TTS: Response statusText:', response.statusText);
                        console.log('TTS: Response headers:', Object.fromEntries(response.headers.entries()));
                        console.log('TTS: Response type:', response.type);
                        console.log('TTS: Response url:', response.url);
                        
                        if (!response.ok) {
                            console.error('TTS: Server returned an HTTP error!', response.status, response.statusText);
                            throw new Error(`Server error: ${response.status} ${response.statusText}`);
                        }
                        
                        console.log('TTS: Response OK, proceeding to read stream');
                    } catch (error) {
                        console.error('TTS: Fetch threw an error:', error);
                        console.error('TTS: Error name:', error.name);
                        console.error('TTS: Error message:', error.message);
                        console.error('TTS: Error stack:', error.stack);
                        throw error;
                    }
                    
                    const reader = response.body.getReader();
                    const textDecoder = new TextDecoder();
                    let buffer = '';
                    let audioChunksProcessed = 0;
                    
                    console.log('TTS: Starting to read stream...');
                    
                    const pump = async () => {
                        try {
                            while (true) {
                                const { done, value } = await reader.read();
                                
                                if (done) {
                                    console.log('TTS: Stream complete, processing final buffer...');
                                    // Process any remaining buffer
                                    if (buffer.trim()) {
                                        try {
                                            const data = JSON.parse(buffer);
                                            if (data.audio) {
                                                const audioData = Uint8Array.from(atob(data.audio), c => c.charCodeAt(0));
                                                console.log('TTS: Final chunk size:', audioData.byteLength);
                                                queue.push(audioData);
                                                processQueue();
                                                audioChunksProcessed++;
                                            }
                                        } catch (e) {
                                            console.error('TTS: Final buffer parse error:', e);
                                        }
                                    }
                                    
                                    // Wait for queue to empty before ending stream
                                    const waitForQueue = () => {
                                        if (queue.length === 0 && !sourceBuffer.updating) {
                                            mediaSource.endOfStream();
                                            console.log(`TTS: Streaming complete! Processed ${audioChunksProcessed} audio chunks`);
                                            showStatus('Speech synthesis complete', 'success');
                                        } else {
                                            setTimeout(waitForQueue, 100);
                                        }
                                    };
                                    waitForQueue();
                                    break;
                                }
                                
                                const text = textDecoder.decode(value, { stream: true });
                                buffer += text;
                                
                                // Process newline-delimited JSON
                                const lines = buffer.split('\n');
                                buffer = lines.pop() || '';
                                
                                for (const line of lines) {
                                    if (!line.trim()) continue;
                                    try {
                                        const data = JSON.parse(line.trim());
                                        if (data.audio) {
                                            audioChunksProcessed++;
                                            
                                            // Convert base64 to Uint8Array
                                            const audioData = Uint8Array.from(atob(data.audio), c => c.charCodeAt(0));
                                            console.log(`TTS: Audio chunk ${audioChunksProcessed} size:`, audioData.byteLength);
                                            
                                            // Add to queue for MediaSource
                                            queue.push(audioData);
                                            processQueue();
                                        }
                                    } catch (e) {
                                        console.error('TTS: JSON parse error:', e);
                                        console.error('TTS: Failed to parse line:', line);
                                    }
                                }
                            }
                        } catch (error) {
                            console.error('TTS: Streaming error:', error);
                            showStatus(`Streaming error: ${error.message}`, 'error');
                        }
                    };
                    
                    pump();
                });
                
            } catch (error) {
                console.error('TTS: Conversion error:', error);
                console.error('TTS: Error details:', {
                    name: error.name,
                    message: error.message,
                    stack: error.stack
                });
                showStatus(`Text-to-speech error: ${error.message}`, 'error');
            }
        }
        
        // Handle page unload
        window.addEventListener('beforeunload', () => {
            if (isTranscribing) {
                stopTranscription();
            }
        });
    </script>
</body>
</html>