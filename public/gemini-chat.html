<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>Gemini 2.5 Flash - Chatbot Interface</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 20px; display: flex; flex-direction: column; height: calc(100vh - 40px); }
        h3 { margin-top: 0; }
        .chat-container { flex: 1; overflow-y: auto; border: 1px solid #ccc; padding: 10px; margin-bottom: 10px; background: #f5f5f5; }
        .message { margin: 10px 0; padding: 8px 12px; border-radius: 5px; }
        .user-message { background: #e3f2fd; margin-left: 20%; text-align: right; }
        .assistant-message { background: #f5f5f5; margin-right: 20%; border: 1px solid #ddd; }
        .assistant-message pre { margin: 0; white-space: pre-wrap; word-wrap: break-word; font-family: monospace; font-size: 12px; }
        .input-container { display: flex; gap: 10px; }
        #userInput { flex: 1; padding: 10px; font-size: 14px; height: 100px; resize: none; overflow-y: auto; font-family: Arial, sans-serif; }
        button { padding: 10px 20px; cursor: pointer; }
        .status { margin: 10px 0; padding: 5px; border-radius: 3px; }
        .success { background: #d4edda; color: #155724; }
        .error { background: #f8d7da; color: #721c24; }
        .info { background: #cce7ff; color: #004085; }
        #timer { font-weight: bold; margin: 10px 0; color: #333; }
        .transcription-container {
            border: 1px solid #ddd;
            padding: 10px;
            margin: 10px 0;
            min-height: 100px;
            max-height: 200px;
            overflow-y: auto;
            background: #f8f9fa;
            font-family: Arial, sans-serif;
            font-size: 14px;
        }
        .interim { color: #666; font-style: italic; }
        .final { color: #000; }
        .mute-controls { margin: 5px 0; display: flex; align-items: center; gap: 10px; }
        .mute-controls input[type="checkbox"] { cursor: pointer; }
        .mute-controls label { cursor: pointer; user-select: none; }
    </style>
</head>
<body>
    <h3>Gemini 2.5 Flash - Chatbot Interface</h3>
    <div id="knowledgeUpload" style="margin-bottom: 20px; padding: 15px; background: #f0f8ff; border: 1px solid #4CAF50; border-radius: 5px;">
        <label for="fileInput" style="display: block; margin-bottom: 10px; font-weight: bold;">Upload Knowledge Base (Text File):</label>
        <input type="file" id="fileInput" accept=".txt" style="margin-bottom: 10px;">
        <div id="fileInfo" style="display: none; margin-top: 10px; padding: 10px; background: #e8f5e9; border-radius: 3px;">
            <strong>Loaded:</strong> <span id="fileName"></span> (<span id="fileSize"></span>)
        </div>
    </div>
    <div id="chatContainer" class="chat-container"></div>
    <div id="timer"></div>
    <!-- Cache info commented out until cachedContentTokenCount is fixed
    <div id="cacheInfo" style="margin: 10px 0; padding: 10px; background: #fff3cd; border: 1px solid #ffc107; border-radius: 3px; display: none;">
        <strong>Cache Stats:</strong> <span id="cacheStats"></span>
    </div>
    -->
    <div id="status" class="status" style="display: none;"></div>
    <div class="mute-controls">
        <input type="checkbox" id="muteCheckbox" checked>
        <label for="muteCheckbox">Mute Transcription</label>
    </div>
    <div class="transcription-container">
        <div id="liveTranscription">
            <span class="interim">Transcription will appear here when unmuted...</span>
        </div>
    </div>
    <div class="input-container">
        <textarea id="userInput" placeholder="Type your message... (press Enter 3 times to send)" disabled></textarea>
        <button onclick="clearChat()">Clear Chat</button>
    </div>
    
    <script>
        let conversationHistory = [];
        let requestStartTime = 0;
        let isStreaming = false;
        let knowledgeBaseContent = '';
        let knowledgeBaseFileName = '';
        let currentAbortController = null;
        let enterPressCount = 0;
        
        // Transcription variables
        let ws = null;
        let microphone = null;
        let isTranscribing = false;
        let utterances = new Map();
        let microphoneStream = null;
        let finalizedTranscription = ''; // Store finalized text separately
        
        const userInput = document.getElementById('userInput');
        const chatContainer = document.getElementById('chatContainer');
        const timerElement = document.getElementById('timer');
        const muteCheckbox = document.getElementById('muteCheckbox');
        const liveTranscription = document.getElementById('liveTranscription');
        
        // Enable input on load
        window.onload = async () => {
            userInput.disabled = false;
            userInput.focus();
            setupFileUpload();
            
            // Request microphone permissions on load
            try {
                console.log('Requesting microphone permissions on page load...');
                microphoneStream = await navigator.mediaDevices.getUserMedia({ 
                    audio: {
                        echoCancellation: true,
                        noiseSuppression: true,
                        sampleRate: 16000
                    }
                });
                console.log('Microphone access granted on page load');
                // Stop the stream immediately - we'll use it later when needed
                microphoneStream.getTracks().forEach(track => track.stop());
                
                // Setup mute checkbox listener
                muteCheckbox.addEventListener('change', handleMuteChange);
            } catch (error) {
                console.error('Failed to get microphone permissions:', error);
                showStatus('Microphone access denied. Voice transcription will not be available.', 'error');
            }
        };
        
        function setupFileUpload() {
            const fileInput = document.getElementById('fileInput');
            fileInput.addEventListener('change', async (event) => {
                const file = event.target.files[0];
                if (file && file.type === 'text/plain') {
                    try {
                        knowledgeBaseContent = await file.text();
                        knowledgeBaseFileName = file.name;
                        
                        // Update UI
                        document.getElementById('fileName').textContent = file.name;
                        document.getElementById('fileSize').textContent = formatFileSize(file.size);
                        document.getElementById('fileInfo').style.display = 'block';
                        
                        showStatus(`Knowledge base "${file.name}" loaded successfully! (${knowledgeBaseContent.length} characters)`, 'success');
                    } catch (error) {
                        showStatus(`Error reading file: ${error.message}`, 'error');
                    }
                } else {
                    showStatus('Please select a text file (.txt)', 'error');
                    event.target.value = '';
                }
            });
        }
        
        function formatFileSize(bytes) {
            if (bytes < 1024) return bytes + ' bytes';
            else if (bytes < 1024 * 1024) return Math.round(bytes / 1024) + ' KB';
            else return Math.round(bytes / (1024 * 1024) * 10) / 10 + ' MB';
        }
        
        function fixIncompleteJSON(partialJSON) {
            // Try to parse as-is first
            try {
                return JSON.parse(partialJSON);
            } catch (e) {
                // Not valid JSON, attempt to fix it
            }
            
            let fixed = partialJSON.trim();
            
            // Check if we're in the middle of a string by counting quotes
            let quoteCount = 0;
            let inString = false;
            let lastChar = '';
            
            for (let i = 0; i < fixed.length; i++) {
                const char = fixed[i];
                if (char === '"' && lastChar !== '\\') {
                    quoteCount++;
                    inString = !inString;
                }
                lastChar = char;
            }
            
            // If we're in the middle of a string, close it
            if (inString) {
                fixed += '"';
            }
            
            // Check if we need to close the array
            if (!fixed.includes('"]')) {
                // Find the last occurrence of sentences array
                const sentencesIndex = fixed.lastIndexOf('"sentences"');
                if (sentencesIndex !== -1) {
                    // Check if array is not closed
                    const afterSentences = fixed.substring(sentencesIndex);
                    if (afterSentences.includes('[') && !afterSentences.includes(']')) {
                        fixed += ']';
                    }
                }
            }
            
            // Check if we need to close the object
            if (!fixed.endsWith('}')) {
                fixed += '}';
            }
            
            // Try to parse the fixed JSON
            try {
                const parsed = JSON.parse(fixed);
                // Add interruption marker if sentences array exists
                if (parsed.sentences && Array.isArray(parsed.sentences)) {
                    parsed.sentences.push('(Response interrupted by user)');
                }
                return parsed;
            } catch (e) {
                // If still can't parse, return a default structure
                return {
                    sentences: ['(Response interrupted by user - partial response could not be recovered)']
                };
            }
        }
        
        document.addEventListener('keydown', (e) => {
            // Always focus textarea if not already focused
            if (document.activeElement !== userInput && !userInput.disabled) {
                userInput.focus();
            }
            
            if (e.key === 'Tab' && document.activeElement === userInput) {
                e.preventDefault();
                const start = userInput.selectionStart;
                const end = userInput.selectionEnd;
                const value = userInput.value;
                userInput.value = value.substring(0, start) + '\t' + value.substring(end);
                userInput.selectionStart = userInput.selectionEnd = start + 1;
                enterPressCount = 0;
            } else if (e.key === 'Enter') {
                if (isStreaming) {
                    // Single Enter to interrupt current stream
                    e.preventDefault();
                    if (currentAbortController) {
                        currentAbortController.abort();
                        enterPressCount = 0;
                    }
                } else if (document.activeElement === userInput) {
                    e.preventDefault();
                    const content = userInput.value;
                    const transcriptionContent = liveTranscription.textContent.trim();
                    
                    // Check if there's content in either typed textarea or transcription
                    const hasTypedContent = content.length > 0;
                    const hasTranscriptionContent = transcriptionContent.length > 0 && 
                        !transcriptionContent.includes('Transcription will appear here') &&
                        !transcriptionContent.includes('Listening...');
                    
                    // If neither has content, do nothing
                    if (!hasTypedContent && !hasTranscriptionContent) {
                        return;
                    }
                    
                    enterPressCount++;
                    
                    if (enterPressCount === 3) {
                        // Submit on third Enter press
                        sendMessage();
                        enterPressCount = 0;
                    } else {
                        // Insert newline for 1st and 2nd Enter press
                        // Only insert newline if the typed textarea has content
                        if (hasTypedContent) {
                            const start = userInput.selectionStart;
                            const end = userInput.selectionEnd;
                            userInput.value = content.substring(0, start) + '\n' + content.substring(end);
                            userInput.selectionStart = userInput.selectionEnd = start + 1;
                        }
                    }
                }
            } else if (e.key === 'Escape') {
                // Toggle mute checkbox on Esc key
                e.preventDefault();
                muteCheckbox.checked = !muteCheckbox.checked;
                // Trigger change event to call handleMuteChange
                muteCheckbox.dispatchEvent(new Event('change'));
            } else {
                // Reset counter on any non-Enter key
                enterPressCount = 0;
            }
        });
        
        function showStatus(message, type = 'info') {
            const statusDiv = document.getElementById('status');
            statusDiv.textContent = message;
            statusDiv.className = `status ${type}`;
            statusDiv.style.display = 'block';
        }
        
        function addMessage(role, content) {
            const messageDiv = document.createElement('div');
            messageDiv.className = `message ${role === 'user' ? 'user-message' : 'assistant-message'}`;
            
            if (role === 'user') {
                messageDiv.textContent = content;
            } else {
                const pre = document.createElement('pre');
                pre.textContent = content;
                messageDiv.appendChild(pre);
            }
            
            chatContainer.appendChild(messageDiv);
            chatContainer.scrollTop = chatContainer.scrollHeight;
        }
        
        async function sendMessage() {
            const typedMessage = userInput.value.trim();
            const transcribedMessage = liveTranscription.textContent.trim();
            
            // Remove the default placeholder text
            const isPlaceholder = transcribedMessage.includes('Transcription will appear here') || 
                                  transcribedMessage.includes('Listening...');
            const cleanTranscription = isPlaceholder ? '' : transcribedMessage;
            
            // Combine messages
            let message = '';
            if (cleanTranscription && typedMessage) {
                message = `Transcribed: ${cleanTranscription}\nTyped: ${typedMessage}`;
            } else if (cleanTranscription) {
                message = `Transcribed: ${cleanTranscription}`;
            } else if (typedMessage) {
                message = typedMessage;
            } else {
                return; // Nothing to send
            }
            
            // Disable input during streaming
            isStreaming = true;
            userInput.disabled = true;
            userInput.value = '';
            userInput.placeholder = 'Press Enter to interrupt...';
            
            // Clear transcription
            clearTranscription();
            
            // Stop transcription during response streaming
            if (isTranscribing) {
                console.log('Stopping transcription for response streaming');
                stopTranscription();
            }
            
            // Add user message to UI and history
            addMessage('user', message);
            conversationHistory.push({
                role: 'user',
                parts: [{ text: message }]
            });
            
            // Start timer
            requestStartTime = performance.now();
            timerElement.textContent = 'Timer: Sending request to Gemini...';
            showStatus('Processing with streaming JSON output...', 'info');
            
            // Create placeholder for assistant response
            const assistantDiv = document.createElement('div');
            assistantDiv.className = 'message assistant-message';
            const pre = document.createElement('pre');
            assistantDiv.appendChild(pre);
            chatContainer.appendChild(assistantDiv);
            
            let fullResponse = '';
            let firstChunkTime = null;
            
            try {
                // Create new AbortController for this request
                currentAbortController = new AbortController();
                
                const requestBody = {
                    contents: conversationHistory,
                    structuredOutput: true
                };
                
                // Build system instruction with or without knowledge base
                const knowledgeFileName = knowledgeBaseContent ? knowledgeBaseFileName : '(none loaded)';
                const knowledgeContent = knowledgeBaseContent || '[No content - no knowledge base file has been uploaded]';
                
                if (knowledgeBaseContent) {
                    console.log('Knowledge base loaded:', knowledgeBaseFileName, 'Length:', knowledgeBaseContent.length);
                } else {
                    console.log('No knowledge base loaded');
                }
                
                requestBody.systemInstruction = `You are a helpful assistant. The user may communicate with you through both typing and voice transcription. When you see messages prefixed with "Transcribed:" it means they spoke the message, and "Typed:" means they typed it. Sometimes messages may contain both transcribed and typed content.

When answering questions, refer to the knowledge base when relevant. If the user asks about something not in the knowledge base, you must state that it's not discussed in the provided content, but then provide an answer based on your general knowledge. For example: "This topic is not discussed in ${knowledgeFileName}, but based on my general knowledge..."

You must respond with a JSON object containing a "sentences" array. IMPORTANT: Each item in the array must contain EXACTLY ONE sentence. Never put multiple sentences in a single array item. Split your response so that each sentence (ending with . ! or ?) is its own array element.

Be conversational, friendly, and helpful in your responses.

Knowledge base content from "${knowledgeFileName}":

${knowledgeContent}`;
                
                const response = await fetch('/api/gemini', {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify(requestBody),
                    signal: currentAbortController.signal
                });
                
                if (!response.ok) {
                    throw new Error(`API Error: ${response.status}`);
                }
                
                const reader = response.body.getReader();
                const decoder = new TextDecoder();
                let buffer = '';
                
                while (true) {
                    const { done, value } = await reader.read();
                    if (done) break;
                    
                    buffer += decoder.decode(value, { stream: true });
                    const lines = buffer.split('\n');
                    buffer = lines.pop() || '';
                    
                    for (const line of lines) {
                        if (line.startsWith('data: ')) {
                            const data = line.slice(6);
                            if (data === '[DONE]') {
                                const totalTime = Math.round(performance.now() - requestStartTime);
                                timerElement.textContent = `Timer: ${totalTime}ms total (first chunk: ${firstChunkTime}ms)`;
                                showStatus('Streaming complete!', 'success');
                                
                                // Add complete response to conversation history
                                conversationHistory.push({
                                    role: 'model',
                                    parts: [{ text: fullResponse }]
                                });
                            } else {
                                try {
                                    const parsed = JSON.parse(data);
                                    if (parsed.text) {
                                        if (!firstChunkTime) {
                                            firstChunkTime = Math.round(performance.now() - requestStartTime);
                                            timerElement.textContent = `Timer: ${firstChunkTime}ms to first chunk (streaming...)`;
                                        }
                                        fullResponse += parsed.text;
                                        // Show the raw response (which should be JSON)
                                        pre.textContent = fullResponse;
                                        chatContainer.scrollTop = chatContainer.scrollHeight;
                                    } else if (parsed.cacheInfo) {
                                        // Cache info commented out until cachedContentTokenCount is fixed
                                        // const cacheDiv = document.getElementById('cacheInfo');
                                        // const cacheStats = document.getElementById('cacheStats');
                                        // cacheStats.textContent = `${parsed.cacheInfo.cachedTokens} cached tokens out of ${parsed.cacheInfo.totalTokens} total (${Math.round(parsed.cacheInfo.cacheHitRate * 100)}% cache hit rate)`;
                                        // cacheDiv.style.display = 'block';
                                    } else if (parsed.error) {
                                        throw new Error(parsed.error);
                                    }
                                } catch (e) {
                                    console.error('Error parsing SSE data:', e);
                                }
                            }
                        }
                    }
                }
            } catch (error) {
                if (error.name === 'AbortError') {
                    // Handle user interruption
                    showStatus('Response interrupted', 'info');
                    const totalTime = Math.round(performance.now() - requestStartTime);
                    timerElement.textContent = `Timer: ${totalTime}ms (interrupted)`;
                    
                    // Fix incomplete JSON and add to conversation history
                    if (fullResponse) {
                        const fixedResponse = fixIncompleteJSON(fullResponse);
                        
                        // Update the display with the fixed JSON
                        pre.textContent = JSON.stringify(fixedResponse, null, 2);
                        
                        // Add the interrupted response to conversation history
                        conversationHistory.push({
                            role: 'model',
                            parts: [{ text: JSON.stringify(fixedResponse) }]
                        });
                    } else {
                        // No response received before interruption
                        const emptyResponse = { sentences: ['(Response interrupted before any content was received)'] };
                        pre.textContent = JSON.stringify(emptyResponse, null, 2);
                        conversationHistory.push({
                            role: 'model',
                            parts: [{ text: JSON.stringify(emptyResponse) }]
                        });
                    }
                } else {
                    // Handle other errors
                    console.error('Error:', error);
                    showStatus(`Error: ${error.message}`, 'error');
                    timerElement.textContent = 'Timer: Request failed';
                    pre.textContent = `Error: ${error.message}`;
                }
            } finally {
                // Re-enable input and clear abort controller
                isStreaming = false;
                userInput.disabled = false;
                userInput.placeholder = 'Type your message... (press Enter 3 times to send)';
                userInput.focus();
                currentAbortController = null;
                enterPressCount = 0;
                
                // Restart transcription if unmuted
                if (!muteCheckbox.checked) {
                    console.log('Restarting transcription after response');
                    // Add longer delay to allow server cleanup
                    setTimeout(() => {
                        console.log('Starting transcription after delay');
                        startTranscription();
                    }, 1000);
                }
            }
        }
        
        function clearChat() {
            conversationHistory = [];
            chatContainer.innerHTML = '';
            timerElement.textContent = '';
            document.getElementById('status').style.display = 'none';
            userInput.focus();
        }
        
        // Handle mute checkbox changes
        function handleMuteChange() {
            const isMuted = muteCheckbox.checked;
            console.log('Mute checkbox changed, muted:', isMuted);
            
            if (isMuted) {
                // Stop transcription if running
                if (isTranscribing) {
                    stopTranscription();
                }
            } else {
                // Start transcription if not streaming a response
                if (!isStreaming) {
                    startTranscription();
                }
            }
        }
        
        // Start transcription with retry capability
        async function startTranscription(retryCount = 0) {
            // Prevent overlapping connections
            if (ws && (ws.readyState === WebSocket.CONNECTING || ws.readyState === WebSocket.OPEN)) {
                console.log('WebSocket connection already exists, skipping...');
                return;
            }
            
            try {
                console.log(`Starting transcription... (attempt ${retryCount + 1})`);
                showStatus('Connecting to transcription server...', 'info');
                
                // Don't clear transcription content - preserve existing text
                // Only show "Listening..." if there's no existing content
                if (!liveTranscription.textContent.trim() || 
                    liveTranscription.textContent.includes('Transcription will appear here')) {
                    liveTranscription.innerHTML = '<span class="interim">Listening...</span>';
                }
                
                // Connect to server WebSocket
                const protocol = window.location.protocol === 'https:' ? 'wss:' : 'ws:';
                const wsUrl = `${protocol}//${window.location.hostname}:3000/api/speech-to-text`;
                console.log('Attempting to connect to:', wsUrl);
                
                ws = new WebSocket(wsUrl);
                console.log('WebSocket created, state:', ws.readyState);
                
                ws.onopen = () => {
                    console.log('WebSocket opened successfully');
                    showStatus('Connected to transcription server', 'success');
                    
                    // Send configuration to start Deepgram connection
                    const config = {
                        action: 'start',
                        model: 'nova-3-medical'
                    };
                    ws.send(JSON.stringify(config));
                    
                    // Setup microphone after connection
                    setTimeout(() => setupMicrophone(), 100);
                };
                
                ws.onmessage = (event) => {
                    console.log('Received message from server:', event.data);
                    try {
                        const data = JSON.parse(event.data);
                        
                        if (data.type === 'status') {
                            console.log('Status message:', data.message);
                            showStatus(data.message, 'info');
                        } else if (data.type === 'error') {
                            console.error('Error from server:', data.message);
                            showStatus(`Transcription error: ${data.message}`, 'error');
                            stopTranscription();
                        } else {
                            // Handle transcription data
                            console.log('Transcription data:', data);
                            updateTranscription(data);
                        }
                    } catch (error) {
                        console.error('Error parsing message:', error);
                    }
                };
                
                ws.onerror = (error) => {
                    console.error('WebSocket error:', error);
                    console.error('WebSocket state on error:', ws.readyState);
                    
                    // Retry with exponential backoff
                    if (retryCount < 3) {
                        const retryDelay = Math.pow(2, retryCount) * 1000; // 1s, 2s, 4s
                        console.log(`Retrying connection in ${retryDelay}ms...`);
                        showStatus(`Connection failed, retrying in ${retryDelay/1000}s...`, 'warning');
                        
                        // Clean up current connection
                        if (ws) {
                            ws.close();
                            ws = null;
                        }
                        isTranscribing = false;
                        
                        setTimeout(() => {
                            if (!isStreaming && !muteCheckbox.checked) {
                                startTranscription(retryCount + 1);
                            }
                        }, retryDelay);
                    } else {
                        console.error('Max retries reached, giving up');
                        showStatus('Failed to connect after multiple attempts', 'error');
                        stopTranscription();
                    }
                };
                
                ws.onclose = (event) => {
                    console.log('WebSocket closed. Code:', event.code, 'Reason:', event.reason);
                    console.log('Clean close?', event.wasClean);
                    
                    // Clean up WebSocket reference
                    ws = null;
                    isTranscribing = false;
                    
                    if (event.code === 1006 && retryCount < 3) {
                        console.error('Abnormal closure - attempting retry');
                        const retryDelay = Math.pow(2, retryCount) * 1000;
                        showStatus(`Connection lost, retrying in ${retryDelay/1000}s...`, 'warning');
                        
                        setTimeout(() => {
                            if (!isStreaming && !muteCheckbox.checked) {
                                startTranscription(retryCount + 1);
                            }
                        }, retryDelay);
                    } else if (event.code === 1006) {
                        console.error('Abnormal closure - connection lost without proper close handshake');
                        showStatus('Transcription connection lost abnormally', 'error');
                    } else if (isTranscribing) {
                        showStatus('Transcription connection closed', 'warning');
                    }
                };
                
                // Update state
                isTranscribing = true;
                
            } catch (error) {
                console.error('Failed to start transcription:', error);
                showStatus(`Failed to start transcription: ${error.message}`, 'error');
                stopTranscription();
            }
        }
        
        // Setup microphone
        async function setupMicrophone() {
            try {
                console.log('Setting up microphone...');
                const stream = await navigator.mediaDevices.getUserMedia({ 
                    audio: {
                        echoCancellation: true,
                        noiseSuppression: true,
                        sampleRate: 16000
                    }
                });
                console.log('Microphone stream obtained');
                
                const mimeType = 'audio/webm';
                console.log('Using mime type:', mimeType);
                
                microphone = new MediaRecorder(stream, {
                    mimeType: mimeType
                });
                
                microphone.ondataavailable = (event) => {
                    if (event.data.size > 0 && ws && ws.readyState === WebSocket.OPEN) {
                        console.log('Sending audio chunk:', event.data.size, 'bytes');
                        ws.send(event.data);
                    }
                };
                
                microphone.onerror = (error) => {
                    console.error('MediaRecorder error:', error);
                    showStatus('Microphone error', 'error');
                };
                
                // Start recording with 100ms chunks
                microphone.start(100);
                console.log('MediaRecorder started');
                showStatus('Listening...', 'success');
                
            } catch (error) {
                console.error('Microphone setup error:', error);
                showStatus('Microphone access denied', 'error');
                stopTranscription();
            }
        }
        
        // Stop transcription
        function stopTranscription() {
            console.log('Stopping transcription...');
            
            // Stop microphone
            if (microphone && microphone.state !== 'inactive') {
                microphone.stop();
                microphone.stream.getTracks().forEach(track => track.stop());
                microphone = null;
                console.log('Microphone stopped');
            }
            
            // Close WebSocket connection
            if (ws) {
                if (ws.readyState === WebSocket.OPEN) {
                    ws.send(JSON.stringify({ action: 'stop' }));
                }
                ws.close();
                ws = null;
                console.log('WebSocket closed');
            }
            
            // Update state
            isTranscribing = false;
            
            // Don't clear transcription - let it persist
            // Only show placeholder if there's truly no content
            if (utterances.size === 0 && !finalizedTranscription) {
                liveTranscription.innerHTML = '<span class="interim">Transcription will appear here when unmuted...</span>';
            }
        }
        
        // Update transcription display
        function updateTranscription(data) {
            if (data.type === 'SpeechStarted' || data.type === 'UtteranceEnd') {
                return;
            }
            
            if (!data.channel || !data.channel.alternatives || !data.channel.alternatives[0]) {
                console.log('Unexpected data format:', data);
                return;
            }
            
            const transcript = data.channel.alternatives[0].transcript;
            if (transcript === "") return;
            
            const isFinal = data.is_final;
            const utteranceId = data.start;
            
            if (utteranceId === undefined || utteranceId === null) {
                console.warn("Received transcript without a start time, cannot process.", data);
                return;
            }
            
            if (isFinal) {
                // If this utterance was previously interim, remove it from the map
                if (utterances.has(utteranceId)) {
                    utterances.delete(utteranceId);
                }
                // Append finalized text
                if (finalizedTranscription && !finalizedTranscription.endsWith(' ')) {
                    finalizedTranscription += ' ';
                }
                finalizedTranscription += transcript;
            } else {
                // Store or update interim transcription
                utterances.set(utteranceId, {
                    transcript: transcript,
                    is_final: false
                });
            }
            
            // Build display content: finalized text + interim utterances
            let displayContent = '';
            
            // Add finalized transcription
            if (finalizedTranscription) {
                displayContent += `<span class="final">${finalizedTranscription}</span>`;
            }
            
            // Add interim utterances (sorted by ID for consistency within current session)
            const sortedUtterances = Array.from(utterances.entries()).sort((a, b) => a[0] - b[0]);
            for (const [id, utterance] of sortedUtterances) {
                if (!utterance.is_final) {
                    if (displayContent && !displayContent.endsWith(' ')) {
                        displayContent += ' ';
                    }
                    displayContent += `<span class="interim">${utterance.transcript}</span>`;
                }
            }
            
            // Update display
            liveTranscription.innerHTML = displayContent || '<span class="interim">Listening...</span>';
            liveTranscription.scrollTop = liveTranscription.scrollHeight;
        }
        
        // Clear transcription
        function clearTranscription() {
            liveTranscription.innerHTML = '<span class="interim">Transcription will appear here when unmuted...</span>';
            utterances.clear();
            finalizedTranscription = '';
        }
        
        // Handle page unload
        window.addEventListener('beforeunload', () => {
            if (isTranscribing) {
                stopTranscription();
            }
        });
    </script>
</body>
</html>