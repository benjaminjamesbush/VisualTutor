<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>Deepgram STT Test</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 20px; }
        button { margin: 5px; padding: 10px; cursor: pointer; }
        .status { margin: 10px 0; padding: 5px; border-radius: 3px; }
        .success { background: #d4edda; color: #155724; }
        .error { background: #f8d7da; color: #721c24; }
        .info { background: #cce7ff; color: #004085; }
        .warning { background: #fff3cd; color: #856404; }
        .section { margin: 20px 0; padding: 15px; border: 1px solid #ddd; border-radius: 5px; }
        h3 { margin-top: 0; }
        .transcription-container {
            border: 1px solid #ddd;
            padding: 10px;
            margin: 10px 0;
            min-height: 100px;
            max-height: 300px;
            overflow-y: auto;
            background: #f8f9fa;
        }
        .interim { color: #666; font-style: italic; }
        .final { color: #000; }
        #timer { font-weight: bold; margin: 10px 0; color: #333; }
        .controls { margin: 10px 0; }
        select { padding: 5px; margin-left: 10px; }
    </style>
    <!-- Load Deepgram SDK from CDN -->
    <script src="https://cdn.jsdelivr.net/npm/@deepgram/sdk"></script>
</head>
<body>
    <h1>Deepgram Speech-to-Text Test Page</h1>
    
    <!-- WebSocket Live Transcription -->
    <div class="section">
        <h3>Live Transcription (WebSocket)</h3>
        <div class="controls">
            <button id="wsButton" onclick="toggleTranscription()" style="background: #007bff; color: white;">
                Start Live Transcription
            </button>
            <button onclick="clearTranscription()">Clear</button>
            <select id="deepgramModel">
                <option value="nova-3-medical" selected>Nova 3 Medical</option>
                <option value="nova-3">Nova 3 (General)</option>
                <option value="nova-2">Nova 2</option>
                <option value="nova">Nova</option>
            </select>
        </div>
        <div id="wsStatus" class="status" style="display: none;"></div>
        <div id="timer" style="display: none;"></div>
        <div class="transcription-container">
            <div id="liveTranscription">
                <span class="interim">Waiting to start...</span>
            </div>
        </div>
        
        <h4>Final Transcription (for LLM):</h4>
        <textarea id="finalTranscription" readonly rows="6" style="width: 100%; margin: 10px 0;" 
                  placeholder="Final transcription will appear here after you stop recording..."></textarea>
    </div>

    <script>
        let connection = null;
        let microphone = null;
        let isTranscribing = false;
        let transcriptionStartTime = 0;
        let timerInterval = null;
        let finalTranscripts = []; // Store all final transcripts
        let utteranceFinals = {}; // Track finals by utterance ID

        // Get API key from server
        async function getApiKey() {
            try {
                const response = await fetch('/api/deepgram/authenticate');
                if (!response.ok) {
                    throw new Error('Failed to authenticate');
                }
                const data = await response.json();
                return data.key;
            } catch (error) {
                console.error('Authentication error:', error);
                throw error;
            }
        }

        // Toggle transcription
        async function toggleTranscription() {
            if (!isTranscribing) {
                await startTranscription();
            } else {
                stopTranscription();
            }
        }

        // Start transcription
        async function startTranscription() {
            try {
                showStatus('wsStatus', 'Initializing...', 'info');
                
                // Clear previous final transcripts
                finalTranscripts = [];
                utteranceFinals = {};
                document.getElementById('finalTranscription').value = '';
                
                // Get API key
                const apiKey = await getApiKey();
                
                // Initialize Deepgram client
                const { createClient, LiveTranscriptionEvents } = deepgram;
                const _deepgram = createClient(apiKey);
                
                // Get selected model
                const model = document.getElementById('deepgramModel').value;
                
                // Create live connection with options
                connection = _deepgram.listen.live({
                    model: model,
                    language: 'en-US',
                    smart_format: true,
                    punctuate: true,
                    interim_results: true,
                    utterance_end_ms: 1000,
                    vad_events: true,
                });

                // Set up connection event handlers
                connection.on(LiveTranscriptionEvents.Open, () => {
                    showStatus('wsStatus', 'Connected to Deepgram', 'success');
                    setupMicrophone();
                });

                connection.on(LiveTranscriptionEvents.Transcript, (data) => {
                    updateTranscription(data);
                });

                connection.on(LiveTranscriptionEvents.Error, (error) => {
                    console.error('Deepgram error:', error);
                    showStatus('wsStatus', 'Connection error', 'error');
                    stopTranscription();
                });

                connection.on(LiveTranscriptionEvents.Close, () => {
                    showStatus('wsStatus', 'Connection closed', 'warning');
                    stopTranscription();
                });

                // Update UI
                isTranscribing = true;
                const wsButton = document.getElementById('wsButton');
                wsButton.textContent = 'Stop Live Transcription';
                wsButton.style.background = '#dc3545';
                
                // Start timer
                transcriptionStartTime = Date.now();
                document.getElementById('timer').style.display = 'block';
                updateTimer();
                
            } catch (error) {
                console.error('Failed to start transcription:', error);
                showStatus('wsStatus', `Error: ${error.message}`, 'error');
                stopTranscription();
            }
        }

        // Setup microphone
        async function setupMicrophone() {
            try {
                const stream = await navigator.mediaDevices.getUserMedia({ 
                    audio: {
                        echoCancellation: true,
                        noiseSuppression: true,
                        sampleRate: 16000
                    }
                });

                microphone = new MediaRecorder(stream, {
                    mimeType: 'audio/webm'
                });

                microphone.ondataavailable = (event) => {
                    // Only send data if we have data and connection is open
                    if (event.data.size > 0 && connection && connection.getReadyState() === 1) {
                        connection.send(event.data);
                    }
                };

                // Start recording with 100ms chunks
                microphone.start(100);
                
            } catch (error) {
                console.error('Microphone error:', error);
                showStatus('wsStatus', 'Microphone access denied', 'error');
                stopTranscription();
            }
        }

        // Stop transcription
        function stopTranscription() {
            // Stop microphone
            if (microphone && microphone.state !== 'inactive') {
                microphone.stop();
                microphone.stream.getTracks().forEach(track => track.stop());
                microphone = null;
            }

            // Close connection
            if (connection) {
                connection.finish();
                connection = null;
            }

            // Update UI
            isTranscribing = false;
            const wsButton = document.getElementById('wsButton');
            wsButton.textContent = 'Start Live Transcription';
            wsButton.style.background = '#007bff';
            
            // Stop timer
            if (timerInterval) {
                clearInterval(timerInterval);
                timerInterval = null;
            }
            
            // Display final transcription
            if (finalTranscripts.length > 0) {
                document.getElementById('finalTranscription').value = finalTranscripts.join(' ');
            }
        }

        // Update transcription display
        function updateTranscription(data) {
            if (!data.channel || !data.channel.alternatives || !data.channel.alternatives[0]) {
                return;
            }

            const transcript = data.channel.alternatives[0].transcript;
            if (!transcript) return;

            const transcriptionDiv = document.getElementById('liveTranscription');
            const isFinal = data.is_final;
            const speechFinal = data.speech_final;
            
            // Create or update utterance
            let utteranceId = `utterance-${data.start || Date.now()}`;
            let span = document.getElementById(utteranceId);
            
            if (!span) {
                span = document.createElement('span');
                span.id = utteranceId;
                transcriptionDiv.appendChild(span);
                transcriptionDiv.appendChild(document.createTextNode(' '));
            }

            span.textContent = transcript;
            span.className = isFinal ? 'final' : 'interim';
            
            // Collect final transcripts
            if (isFinal) {
                // Track by utterance to avoid duplicates
                const utteranceKey = `${data.start}`;
                if (!utteranceFinals[utteranceKey]) {
                    utteranceFinals[utteranceKey] = transcript;
                    finalTranscripts.push(transcript);
                }
            }

            // Auto-scroll to bottom
            transcriptionDiv.scrollTop = transcriptionDiv.scrollHeight;
        }

        // Clear transcription
        function clearTranscription() {
            document.getElementById('liveTranscription').innerHTML = '<span class="interim">Waiting to start...</span>';
            document.getElementById('finalTranscription').value = '';
            finalTranscripts = [];
            utteranceFinals = {};
        }

        // Update timer
        function updateTimer() {
            if (!isTranscribing) return;
            
            const elapsed = Math.floor((Date.now() - transcriptionStartTime) / 1000);
            const minutes = Math.floor(elapsed / 60);
            const seconds = elapsed % 60;
            
            document.getElementById('timer').textContent = 
                `Elapsed: ${minutes}:${seconds.toString().padStart(2, '0')}`;
            
            timerInterval = setTimeout(updateTimer, 1000);
        }

        // Show status message
        function showStatus(elementId, message, type = 'info') {
            const statusDiv = document.getElementById(elementId);
            statusDiv.textContent = message;
            statusDiv.className = `status ${type}`;
            statusDiv.style.display = 'block';
        }

        // Handle page unload
        window.addEventListener('beforeunload', () => {
            if (isTranscribing) {
                stopTranscription();
            }
        });
    </script>
</body>
</html>